{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7497725,"sourceType":"datasetVersion","datasetId":4365847}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:01:43.732680Z\",\"iopub.execute_input\":\"2024-01-28T11:01:43.733401Z\",\"iopub.status.idle\":\"2024-01-28T11:01:43.743939Z\",\"shell.execute_reply.started\":\"2024-01-28T11:01:43.733364Z\",\"shell.execute_reply\":\"2024-01-28T11:01:43.742719Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:01:48.704311Z\",\"iopub.execute_input\":\"2024-01-28T11:01:48.704836Z\",\"iopub.status.idle\":\"2024-01-28T11:01:50.111988Z\",\"shell.execute_reply.started\":\"2024-01-28T11:01:48.704794Z\",\"shell.execute_reply\":\"2024-01-28T11:01:50.110259Z\"}}\ncc=pd.read_csv(\"/kaggle/input/creditcardfraud/Creditcard_data.csv\")\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nx=cc.drop('Class',axis=1)\ny=cc['Class']\n\nros = RandomOverSampler(random_state=42)\nx_ros, y_ros = ros.fit_resample(x, y)\n\n\n\n# Create a new balanced dataframe\ncc_balanced= pd.concat([x_ros, y_ros], axis=1)\n\n# Save the balanced dataframe to a new CSV file\ncc_balanced.to_csv('Balanced_data.csv', index=False)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:03:06.726790Z\",\"iopub.execute_input\":\"2024-01-28T11:03:06.727275Z\",\"iopub.status.idle\":\"2024-01-28T11:03:06.777271Z\",\"shell.execute_reply.started\":\"2024-01-28T11:03:06.727218Z\",\"shell.execute_reply\":\"2024-01-28T11:03:06.775741Z\"}}\ncc_balanced\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:03:39.590437Z\",\"iopub.execute_input\":\"2024-01-28T11:03:39.590862Z\",\"iopub.status.idle\":\"2024-01-28T11:03:39.600152Z\",\"shell.execute_reply.started\":\"2024-01-28T11:03:39.590830Z\",\"shell.execute_reply\":\"2024-01-28T11:03:39.598505Z\"}}\n#Creating a random sample using random sampling\nnp.random.seed(0)\n\nsample_size=int((pow(1.96,2)*0.5*0.5)/0.0025)\n\nrandom_sample = cc_balanced.sample(n=sample_size, random_state=0)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:03:51.661832Z\",\"iopub.execute_input\":\"2024-01-28T11:03:51.662358Z\",\"iopub.status.idle\":\"2024-01-28T11:03:51.695455Z\",\"shell.execute_reply.started\":\"2024-01-28T11:03:51.662324Z\",\"shell.execute_reply\":\"2024-01-28T11:03:51.694323Z\"}}\nrandom_sample.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:06:18.276032Z\",\"iopub.execute_input\":\"2024-01-28T11:06:18.277140Z\",\"iopub.status.idle\":\"2024-01-28T11:06:18.882501Z\",\"shell.execute_reply.started\":\"2024-01-28T11:06:18.277099Z\",\"shell.execute_reply\":\"2024-01-28T11:06:18.880942Z\"}}\n#Now we will aplly 5 different models to our random sample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_random= random_sample.drop(columns=['Class'])\ny_random = random_sample['Class']\nx_random_train, x_random_test,y_random_train, y_random_test = train_test_split(x_random,y_random, random_state=132, \n                                   test_size=0.25, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()  \n}\nacc_l=[]\n\nfor name, model in models.items():\n    model.fit(x_random_train,y_random_train)\n    y_pred = model.predict(x_random_test)\n    acc = accuracy_score(y_random_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l.append(acc)\nacc_m=pd.DataFrame(acc_l,index=['Logistic Regression','Gradient Boosting','Naive bayes Classifier','Decision Tree','Random Forest'])\nacc_m.rename(columns={ acc_m.columns[0]: \"Random_Sampling\" }, inplace = True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:07:01.527216Z\",\"iopub.execute_input\":\"2024-01-28T11:07:01.527719Z\",\"iopub.status.idle\":\"2024-01-28T11:07:01.540704Z\",\"shell.execute_reply.started\":\"2024-01-28T11:07:01.527684Z\",\"shell.execute_reply\":\"2024-01-28T11:07:01.538952Z\"}}\nacc_m\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:08:31.116652Z\",\"iopub.execute_input\":\"2024-01-28T11:08:31.117157Z\",\"iopub.status.idle\":\"2024-01-28T11:08:31.125683Z\",\"shell.execute_reply.started\":\"2024-01-28T11:08:31.117117Z\",\"shell.execute_reply\":\"2024-01-28T11:08:31.124364Z\"}}\nn = len(cc_balanced)\nn\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:12:47.098845Z\",\"iopub.execute_input\":\"2024-01-28T11:12:47.099343Z\",\"iopub.status.idle\":\"2024-01-28T11:12:47.168572Z\",\"shell.execute_reply.started\":\"2024-01-28T11:12:47.099309Z\",\"shell.execute_reply\":\"2024-01-28T11:12:47.167357Z\"}}\nimport pandas as pd\nimport numpy as np\nimport math\n\n# Read the CSV file into a DataFrame\n\n\n# Separate the feature matrix X and the target variable y\nX = cc_balanced.drop(columns=['Class'])\ny = cc_balanced['Class']\n\n# Determine the number of strata (in this case, we use a binary target variable, so there are two strata)\nnum_strata = 2\n\n# Initialize an empty list to store the stratified samples\nsamples = []\n\n# Loop over each stratum\nfor i in range(num_strata):\n    # Subset the data to include only the observations in the current stratum\n    stratum_data = cc_balanced[cc_balanced['Class'] ==i]\n    \n    # Calculate the sample size for the current stratum\n    stratum_size = len(stratum_data)\n    population_size=len(cc_balanced)\n    p=0.5\n    error=0.05\n    z_score = 1.96  # for a 95% confidence level\n    p = stratum_size / population_size\n    q = 1 - p\n    n = int((z_score**2 * p * q * population_size) / ((z_score**2 * p * q) + (error**2 * (population_size-1))))\n    \n    \n    # If the calculated sample size for the current stratum is greater than the number of observations in the stratum, set the sample size to the number of observations\n    if n > stratum_size:\n        n = stratum_size\n    \n    # Randomly select observations from the current stratum to include in the sample\n    sample_indices = np.random.choice(stratum_data.index, size=n, replace=False)\n    stratum_sample = stratum_data.loc[sample_indices]\n    \n    # Add the current stratum sample to the list of stratified samples\n    samples.append(stratum_sample)\n\n# Combine the stratified samples into a single DataFrame\nstratified_sample = pd.concat(samples)\n\n# Write the stratified sample to a new CSV file\nstratified_sample.to_csv('stratified_dataset.csv', index=False)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:13:42.573805Z\",\"iopub.execute_input\":\"2024-01-28T11:13:42.574290Z\",\"iopub.status.idle\":\"2024-01-28T11:13:42.617920Z\",\"shell.execute_reply.started\":\"2024-01-28T11:13:42.574241Z\",\"shell.execute_reply\":\"2024-01-28T11:13:42.616695Z\"}}\nstratified_sample\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:14:57.448595Z\",\"iopub.execute_input\":\"2024-01-28T11:14:57.449150Z\",\"iopub.status.idle\":\"2024-01-28T11:14:58.302566Z\",\"shell.execute_reply.started\":\"2024-01-28T11:14:57.449108Z\",\"shell.execute_reply\":\"2024-01-28T11:14:58.301380Z\"}}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_stratified= stratified_sample.drop(columns=['Class'])\ny_stratified = stratified_sample['Class']\nx_stratified_train, x_stratified_test,y_stratified_train, y_stratified_test = train_test_split(x_stratified,y_stratified, random_state=121, \n                                   test_size=0.3, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()\n}\nacc_l3=[]\nfor name, model in models.items():\n    model.fit(x_stratified_train,y_stratified_train)\n    y_pred = model.predict(x_stratified_test)\n    acc = accuracy_score(y_stratified_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l3.append(acc)\nacc_m['Stratified_Sampling']=acc_l3\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:15:07.950686Z\",\"iopub.execute_input\":\"2024-01-28T11:15:07.951192Z\",\"iopub.status.idle\":\"2024-01-28T11:15:07.966291Z\",\"shell.execute_reply.started\":\"2024-01-28T11:15:07.951146Z\",\"shell.execute_reply\":\"2024-01-28T11:15:07.964869Z\"}}\nacc_m\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:15:46.812502Z\",\"iopub.execute_input\":\"2024-01-28T11:15:46.812935Z\",\"iopub.status.idle\":\"2024-01-28T11:15:46.820752Z\",\"shell.execute_reply.started\":\"2024-01-28T11:15:46.812903Z\",\"shell.execute_reply\":\"2024-01-28T11:15:46.819152Z\"}}\n# Set the sampling interval \"k\" as the square root of the number of rows in the dataset\nk = int(math.sqrt(n))\ncc_systematic_sample = cc_balanced.iloc[::k]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:15:57.824708Z\",\"iopub.execute_input\":\"2024-01-28T11:15:57.825121Z\",\"iopub.status.idle\":\"2024-01-28T11:15:57.870912Z\",\"shell.execute_reply.started\":\"2024-01-28T11:15:57.825092Z\",\"shell.execute_reply\":\"2024-01-28T11:15:57.869667Z\"}}\ncc_systematic_sample\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:16:32.129068Z\",\"iopub.execute_input\":\"2024-01-28T11:16:32.130349Z\",\"iopub.status.idle\":\"2024-01-28T11:16:32.535936Z\",\"shell.execute_reply.started\":\"2024-01-28T11:16:32.130291Z\",\"shell.execute_reply\":\"2024-01-28T11:16:32.534593Z\"}}\n#Now we will aplly 5 different models to our systematic sample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_systematic= cc_systematic_sample.drop(columns=['Class'])\ny_systematic = cc_systematic_sample['Class']\nx_systematic_train, x_systematic_test,y_systematic_train, y_systematic_test = train_test_split(x_systematic,y_systematic, random_state=121, \n                                   test_size=0.3, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()  \n}\nacc_l2=[]\nfor name, model in models.items():\n    model.fit(x_systematic_train,y_systematic_train)\n    y_pred = model.predict(x_systematic_test)\n    acc = accuracy_score(y_systematic_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l2.append(acc)\nacc_m['Systematic_Sampling']=acc_l2\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:16:39.593347Z\",\"iopub.execute_input\":\"2024-01-28T11:16:39.593864Z\",\"iopub.status.idle\":\"2024-01-28T11:16:39.608958Z\",\"shell.execute_reply.started\":\"2024-01-28T11:16:39.593830Z\",\"shell.execute_reply\":\"2024-01-28T11:16:39.606950Z\"}}\nacc_m\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:17:04.483606Z\",\"iopub.execute_input\":\"2024-01-28T11:17:04.484082Z\",\"iopub.status.idle\":\"2024-01-28T11:17:04.569288Z\",\"shell.execute_reply.started\":\"2024-01-28T11:17:04.484048Z\",\"shell.execute_reply\":\"2024-01-28T11:17:04.568324Z\"}}\nfrom imblearn.over_sampling import SMOTE\nX = cc.drop(columns=['Class'])\ny = cc['Class']\nsmote = SMOTE()\n\n# fit predictor and target variable\nx_smote, y_smote = smote.fit_resample(x, y)\nsmote_random = pd.concat((pd.DataFrame(x_smote),pd.DataFrame(y_smote)),axis=1)\nnp.random.seed(0)\n\n#using the formula\nsample_size=int((pow(1.96,2)*0.5*0.5)/0.0025)\n\nsmote_random_sample = smote_random.sample(n=sample_size, random_state=0)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:17:51.650854Z\",\"iopub.execute_input\":\"2024-01-28T11:17:51.651436Z\",\"iopub.status.idle\":\"2024-01-28T11:17:51.694526Z\",\"shell.execute_reply.started\":\"2024-01-28T11:17:51.651394Z\",\"shell.execute_reply\":\"2024-01-28T11:17:51.693481Z\"}}\nsmote_random_sample\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:18:15.426145Z\",\"iopub.execute_input\":\"2024-01-28T11:18:15.427755Z\",\"iopub.status.idle\":\"2024-01-28T11:18:16.147937Z\",\"shell.execute_reply.started\":\"2024-01-28T11:18:15.427708Z\",\"shell.execute_reply\":\"2024-01-28T11:18:16.146419Z\"}}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_smote= smote_random_sample.drop(columns=['Class'])\ny_smote = smote_random_sample['Class']\nx_smote_train, x_smote_test,y_smote_train, y_smote_test = train_test_split(x_smote,y_smote, random_state=121, \n                                   test_size=0.3, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n     \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()  \n}\nacc_l5=[]\nfor name, model in models.items():\n    model.fit(x_smote_train,y_smote_train)\n    y_pred = model.predict(x_smote_test)\n    acc = accuracy_score(y_smote_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l5.append(acc)\nacc_m['SMOTE']=acc_l5\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:18:20.407750Z\",\"iopub.execute_input\":\"2024-01-28T11:18:20.408301Z\",\"iopub.status.idle\":\"2024-01-28T11:18:20.424932Z\",\"shell.execute_reply.started\":\"2024-01-28T11:18:20.408237Z\",\"shell.execute_reply\":\"2024-01-28T11:18:20.423837Z\"}}\nacc_m\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:18:42.725413Z\",\"iopub.execute_input\":\"2024-01-28T11:18:42.725956Z\",\"iopub.status.idle\":\"2024-01-28T11:18:42.736490Z\",\"shell.execute_reply.started\":\"2024-01-28T11:18:42.725918Z\",\"shell.execute_reply\":\"2024-01-28T11:18:42.734618Z\"}}\nnp.random.seed(0)\n\nsample_size=500\ncon_sample = cc_balanced.sample(n=sample_size, random_state=91)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:18:51.883060Z\",\"iopub.execute_input\":\"2024-01-28T11:18:51.883525Z\",\"iopub.status.idle\":\"2024-01-28T11:18:51.891371Z\",\"shell.execute_reply.started\":\"2024-01-28T11:18:51.883491Z\",\"shell.execute_reply\":\"2024-01-28T11:18:51.890102Z\"}}\nlen(con_sample)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:19:33.501907Z\",\"iopub.execute_input\":\"2024-01-28T11:19:33.502361Z\",\"iopub.status.idle\":\"2024-01-28T11:19:34.287503Z\",\"shell.execute_reply.started\":\"2024-01-28T11:19:33.502327Z\",\"shell.execute_reply\":\"2024-01-28T11:19:34.286283Z\"}}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_con= con_sample.drop(columns=['Class'])\ny_con = con_sample['Class']\nx_con_train, x_con_test,y_con_train, y_con_test = train_test_split(x_con,y_con, random_state=121, \n                                   test_size=0.3, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n     \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()  \n}\nacc_l4=[]\nfor name, model in models.items():\n    model.fit(x_con_train,y_con_train)\n    y_pred = model.predict(x_con_test)\n    acc = accuracy_score(y_con_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l4.append(acc)\nacc_m['Convinience_Sampling']=acc_l4\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-01-28T11:19:54.170603Z\",\"iopub.execute_input\":\"2024-01-28T11:19:54.171128Z\",\"iopub.status.idle\":\"2024-01-28T11:19:54.189660Z\",\"shell.execute_reply.started\":\"2024-01-28T11:19:54.171087Z\",\"shell.execute_reply\":\"2024-01-28T11:19:54.188336Z\"}}\nFinal_matrix=acc_m\nFinal_matrix\n\n# %% [code]\n# Therefore, Random Forest Classifier gives the highest accuracy with a tie of 1.00 with random sampling\n# and stratified sampling.\n\n# %% [code]\n","metadata":{"_uuid":"c7df4991-c8ca-4d0c-8301-dea00eb3d392","_cell_guid":"ec883ab5-e065-4de1-bee7-17dc126ec3ce","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}